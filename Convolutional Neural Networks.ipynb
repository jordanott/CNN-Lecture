{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageNet](Images/imagenet.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens if we use this for images? (hint: dimensions)\n",
    "![Deep](Images/deep.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurological Origins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Cat](Images/cat.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hubel and Wiesel. Receptive fields of single neurons in the cat's striate cortex. 1959\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Developed by Yann LeCun (1989)\n",
    "* Goal was to build automatic mail-sorting machines\n",
    "* First \"real-world\" application of Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Images/cnn.jpg)  \n",
    "* Weights are shared across space\n",
    "* Provides basis for translational invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Convolution](Images/convolution.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Algorithm](Images/alg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Time Complexity](Images/time.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The run-time complexity of the convolutional operation is described by the above equation. Where $F_{n}$ is the number of filters, $H$ is the height of the input image, $W$ is the width of the image, $f_{h}$ is the height of the filter, $f_{w}$ is the width of the filter, and $f_{d}$ is the depth of the filter. It is important to note that the equation only describes the forward pass in a convolutional layer. **Backpropogation through the network is roughly twice as costly as the forward pass. This is because we must compute the loss with respect to the weights and the loss with respect to the inputs.** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrix](Images/conv_matrix.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU](Images/relu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Very inexpensive\n",
    "* Avoid vanishing gradient\n",
    "* Differentiable at all points except 0\n",
    "* Can kill neurons (Leaky ReLU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Leaky ReLU](Images/leakyrelu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Pooling](Images/pooling.png)\n",
    "* Gives small amount of translational invariance at each level\n",
    "    * Location information of most active neuron is thrown away\n",
    "* Reduces number of inputs to next layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward-pass of a 3-layer neural network:\n",
    "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
    "x = np.random.randn(3, 1) # random input vector of three numbers (3x1)\n",
    "h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)\n",
    "h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)\n",
    "out = np.dot(W3, h2) + b3 # output neuron (1x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y_{i} = \\dfrac{e^{z_{i}}}{\\sum_{j=1}^{N}e^{z_{j}}}$$\n",
    "* Output sums to one\n",
    "* Represent probability distribution across discrete mutually exclusive alternatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Softmax Derivative](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial y_{i}}{\\partial z_{i}} = y_{i}(1-y_{i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_derivative(y):\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy Cost Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C = - \\sum_{j} t_{j} \\log y_{j}$$\n",
    "\n",
    "* Very large gradient when the target value is 1 and the output is 0\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_{i}} = \\sum_{j} \\frac{\\partial C}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial z_{i}} = y_{i} - t_{i} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(y,t):\n",
    "    # y: predicted, t: target\n",
    "    return y - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dropout](Images/dropout.png)\n",
    "\n",
    "* Combat overfitting\n",
    "* Randomly drop connections during training\n",
    "* Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding CNN's\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Weight Visualization](https://www.youtube.com/watch?v=AgkfIQ4IGaM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Conv1](Images/conv1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Conv3](Images/conv3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Conv5](Images/conv5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Class Activation Mapping](http://cnnlocalization.csail.mit.edu/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Code](Images/code.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TensorFlow](https://www.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Open source software library for numerical computation using data flow graphs\n",
    "* Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors)\n",
    "* Allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API\n",
    "* Originally developed by researchers and engineers working on the Google Brain Team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Keras](https://keras.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* High level API\n",
    "* Able to use different libraries as backend\n",
    "    * TensorFlow, Theano, CNTK (Microsoft)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transfer Learning](Images/transfer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Stanford CS 231n](http://cs231n.github.io/convolutional-networks/)  \n",
    "[Keras](https://keras.io/)  \n",
    "[Keras Tutorials](https://github.com/keras-team/keras/tree/master/examples)  \n",
    "[TensorFlow](https://www.tensorflow.org/)  \n",
    "[Deep Visualization Toolbox](https://www.youtube.com/watch?v=AgkfIQ4IGaM)  \n",
    "[Transfer Learning using Keras](https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8)  \n",
    "[Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
